{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "from functions.parse_data import synth_dataloader\n",
    "from multivariate_quantile_regression.network_model import QuantileNetwork\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n",
      "Tensor is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA is available, so let's set default device to GPU\n",
    "    torch.set_default_device(0)\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    # CUDA is not available, so let's use the CPU\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Example usage:\n",
    "tensor = torch.randn(3, 3)  # Create a tensor on the selected device\n",
    "print(\"Tensor is on device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cloud_B02</th>\n",
       "      <th>Cloud_B03</th>\n",
       "      <th>Cloud_B04</th>\n",
       "      <th>Cloud_B05</th>\n",
       "      <th>Cloud_B06</th>\n",
       "      <th>Cloud_B07</th>\n",
       "      <th>Cloud_B08</th>\n",
       "      <th>Cloud_B08A</th>\n",
       "      <th>Cloud_B09</th>\n",
       "      <th>Cloud_B10</th>\n",
       "      <th>...</th>\n",
       "      <th>Clear_B11</th>\n",
       "      <th>Clear_B12</th>\n",
       "      <th>Sat_Zenith_Angle</th>\n",
       "      <th>Sun_Zenith_Angle</th>\n",
       "      <th>Azimuth_Diff_Angle</th>\n",
       "      <th>COT</th>\n",
       "      <th>Cloud_Type</th>\n",
       "      <th>Profile_ID</th>\n",
       "      <th>GOT</th>\n",
       "      <th>Water_Vapor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94195</td>\n",
       "      <td>0.87799</td>\n",
       "      <td>0.92936</td>\n",
       "      <td>0.93407</td>\n",
       "      <td>0.95181</td>\n",
       "      <td>0.96217</td>\n",
       "      <td>0.92871</td>\n",
       "      <td>0.97181</td>\n",
       "      <td>0.49957</td>\n",
       "      <td>0.04136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12946</td>\n",
       "      <td>0.18888</td>\n",
       "      <td>4.53</td>\n",
       "      <td>52.05</td>\n",
       "      <td>167.66</td>\n",
       "      <td>5.897</td>\n",
       "      <td>3</td>\n",
       "      <td>3335</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.30422</td>\n",
       "      <td>0.40100</td>\n",
       "      <td>0.27834</td>\n",
       "      <td>0.57800</td>\n",
       "      <td>1.01964</td>\n",
       "      <td>1.02787</td>\n",
       "      <td>1.00519</td>\n",
       "      <td>1.03599</td>\n",
       "      <td>0.59139</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.71532</td>\n",
       "      <td>0.36823</td>\n",
       "      <td>12.85</td>\n",
       "      <td>41.68</td>\n",
       "      <td>161.91</td>\n",
       "      <td>1.275</td>\n",
       "      <td>2</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.28715</td>\n",
       "      <td>0.25066</td>\n",
       "      <td>0.30366</td>\n",
       "      <td>0.29214</td>\n",
       "      <td>0.34088</td>\n",
       "      <td>0.40079</td>\n",
       "      <td>0.37376</td>\n",
       "      <td>0.48750</td>\n",
       "      <td>0.02092</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86232</td>\n",
       "      <td>0.63915</td>\n",
       "      <td>14.53</td>\n",
       "      <td>79.23</td>\n",
       "      <td>168.52</td>\n",
       "      <td>1.799</td>\n",
       "      <td>1</td>\n",
       "      <td>6796</td>\n",
       "      <td>0.127</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.27146</td>\n",
       "      <td>0.33719</td>\n",
       "      <td>0.19841</td>\n",
       "      <td>0.46411</td>\n",
       "      <td>0.88787</td>\n",
       "      <td>0.89584</td>\n",
       "      <td>0.87746</td>\n",
       "      <td>0.90439</td>\n",
       "      <td>0.51811</td>\n",
       "      <td>0.00561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56307</td>\n",
       "      <td>0.23663</td>\n",
       "      <td>6.54</td>\n",
       "      <td>70.23</td>\n",
       "      <td>165.49</td>\n",
       "      <td>0.519</td>\n",
       "      <td>2</td>\n",
       "      <td>3701</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39689</td>\n",
       "      <td>0.38594</td>\n",
       "      <td>0.32623</td>\n",
       "      <td>0.37338</td>\n",
       "      <td>0.60678</td>\n",
       "      <td>0.66895</td>\n",
       "      <td>0.55343</td>\n",
       "      <td>0.70168</td>\n",
       "      <td>0.01513</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56472</td>\n",
       "      <td>0.20853</td>\n",
       "      <td>8.56</td>\n",
       "      <td>75.15</td>\n",
       "      <td>148.48</td>\n",
       "      <td>8.569</td>\n",
       "      <td>2</td>\n",
       "      <td>6345</td>\n",
       "      <td>0.128</td>\n",
       "      <td>5.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.75592</td>\n",
       "      <td>0.65853</td>\n",
       "      <td>0.70670</td>\n",
       "      <td>0.71369</td>\n",
       "      <td>0.73147</td>\n",
       "      <td>0.74182</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.75003</td>\n",
       "      <td>0.32287</td>\n",
       "      <td>0.00328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92187</td>\n",
       "      <td>0.87515</td>\n",
       "      <td>12.38</td>\n",
       "      <td>73.72</td>\n",
       "      <td>153.05</td>\n",
       "      <td>16.874</td>\n",
       "      <td>3</td>\n",
       "      <td>1419</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.34660</td>\n",
       "      <td>0.30719</td>\n",
       "      <td>0.27459</td>\n",
       "      <td>0.35268</td>\n",
       "      <td>0.72965</td>\n",
       "      <td>0.74705</td>\n",
       "      <td>0.69842</td>\n",
       "      <td>0.75893</td>\n",
       "      <td>0.18345</td>\n",
       "      <td>0.00089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64499</td>\n",
       "      <td>0.27212</td>\n",
       "      <td>14.74</td>\n",
       "      <td>73.05</td>\n",
       "      <td>13.63</td>\n",
       "      <td>3.589</td>\n",
       "      <td>3</td>\n",
       "      <td>424</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.22494</td>\n",
       "      <td>0.33843</td>\n",
       "      <td>0.19615</td>\n",
       "      <td>0.53821</td>\n",
       "      <td>0.98347</td>\n",
       "      <td>0.99256</td>\n",
       "      <td>0.96757</td>\n",
       "      <td>1.00132</td>\n",
       "      <td>0.53669</td>\n",
       "      <td>0.00511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61388</td>\n",
       "      <td>0.28605</td>\n",
       "      <td>8.24</td>\n",
       "      <td>53.20</td>\n",
       "      <td>146.76</td>\n",
       "      <td>0.624</td>\n",
       "      <td>2</td>\n",
       "      <td>3427</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.57982</td>\n",
       "      <td>0.62369</td>\n",
       "      <td>0.57909</td>\n",
       "      <td>0.75147</td>\n",
       "      <td>0.99608</td>\n",
       "      <td>1.04055</td>\n",
       "      <td>0.94017</td>\n",
       "      <td>1.07212</td>\n",
       "      <td>0.43014</td>\n",
       "      <td>0.03328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94942</td>\n",
       "      <td>0.49475</td>\n",
       "      <td>11.57</td>\n",
       "      <td>38.00</td>\n",
       "      <td>117.48</td>\n",
       "      <td>13.909</td>\n",
       "      <td>1</td>\n",
       "      <td>6884</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.83664</td>\n",
       "      <td>0.80620</td>\n",
       "      <td>0.81432</td>\n",
       "      <td>0.78581</td>\n",
       "      <td>0.81063</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.78542</td>\n",
       "      <td>0.85153</td>\n",
       "      <td>0.26374</td>\n",
       "      <td>0.00122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90206</td>\n",
       "      <td>0.89850</td>\n",
       "      <td>9.69</td>\n",
       "      <td>45.83</td>\n",
       "      <td>23.04</td>\n",
       "      <td>24.937</td>\n",
       "      <td>3</td>\n",
       "      <td>7455</td>\n",
       "      <td>0.106</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cloud_B02  Cloud_B03  Cloud_B04  Cloud_B05  Cloud_B06  Cloud_B07  \\\n",
       "0    0.94195    0.87799    0.92936    0.93407    0.95181    0.96217   \n",
       "1    0.30422    0.40100    0.27834    0.57800    1.01964    1.02787   \n",
       "2    0.28715    0.25066    0.30366    0.29214    0.34088    0.40079   \n",
       "3    0.27146    0.33719    0.19841    0.46411    0.88787    0.89584   \n",
       "4    0.39689    0.38594    0.32623    0.37338    0.60678    0.66895   \n",
       "5    0.75592    0.65853    0.70670    0.71369    0.73147    0.74182   \n",
       "6    0.34660    0.30719    0.27459    0.35268    0.72965    0.74705   \n",
       "7    0.22494    0.33843    0.19615    0.53821    0.98347    0.99256   \n",
       "8    0.57982    0.62369    0.57909    0.75147    0.99608    1.04055   \n",
       "9    0.83664    0.80620    0.81432    0.78581    0.81063    0.84108   \n",
       "\n",
       "   Cloud_B08  Cloud_B08A  Cloud_B09  Cloud_B10  ...  Clear_B11  Clear_B12  \\\n",
       "0    0.92871     0.97181    0.49957    0.04136  ...    0.12946    0.18888   \n",
       "1    1.00519     1.03599    0.59139    0.01055  ...    0.71532    0.36823   \n",
       "2    0.37376     0.48750    0.02092    0.00067  ...    0.86232    0.63915   \n",
       "3    0.87746     0.90439    0.51811    0.00561  ...    0.56307    0.23663   \n",
       "4    0.55343     0.70168    0.01513    0.00049  ...    0.56472    0.20853   \n",
       "5    0.71190     0.75003    0.32287    0.00328  ...    0.92187    0.87515   \n",
       "6    0.69842     0.75893    0.18345    0.00089  ...    0.64499    0.27212   \n",
       "7    0.96757     1.00132    0.53669    0.00511  ...    0.61388    0.28605   \n",
       "8    0.94017     1.07212    0.43014    0.03328  ...    0.94942    0.49475   \n",
       "9    0.78542     0.85153    0.26374    0.00122  ...    0.90206    0.89850   \n",
       "\n",
       "   Sat_Zenith_Angle  Sun_Zenith_Angle  Azimuth_Diff_Angle     COT  Cloud_Type  \\\n",
       "0              4.53             52.05              167.66   5.897           3   \n",
       "1             12.85             41.68              161.91   1.275           2   \n",
       "2             14.53             79.23              168.52   1.799           1   \n",
       "3              6.54             70.23              165.49   0.519           2   \n",
       "4              8.56             75.15              148.48   8.569           2   \n",
       "5             12.38             73.72              153.05  16.874           3   \n",
       "6             14.74             73.05               13.63   3.589           3   \n",
       "7              8.24             53.20              146.76   0.624           2   \n",
       "8             11.57             38.00              117.48  13.909           1   \n",
       "9              9.69             45.83               23.04  24.937           3   \n",
       "\n",
       "   Profile_ID    GOT  Water_Vapor  \n",
       "0        3335  0.126         0.35  \n",
       "1        1996  0.126         0.31  \n",
       "2        6796  0.127         4.04  \n",
       "3        3701  0.123         0.22  \n",
       "4        6345  0.128         5.40  \n",
       "5        1419  0.126         0.51  \n",
       "6         424  0.122         0.99  \n",
       "7        3427  0.125         0.35  \n",
       "8        6884  0.122         1.88  \n",
       "9        7455  0.106         1.77  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data and inspect\n",
    "df = synth_dataloader('SMHIdata2')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set columns for X and y (input/output features)\n",
    "X_cols = ['Cloud_B02','Cloud_B03','Cloud_B04','Cloud_B05','Cloud_B06',\n",
    "          'Cloud_B07','Cloud_B08','Cloud_B08A','Cloud_B09','Cloud_B10','Cloud_B11','Cloud_B12',\n",
    "          'Sat_Zenith_Angle','Sun_Zenith_Angle','Azimuth_Diff_Angle']\n",
    "y_cols = ['Clear_B02','Clear_B03','Clear_B04','Clear_B05','Clear_B06',\n",
    "          'Clear_B07','Clear_B08','Clear_B08A','Clear_B09','Clear_B10','Clear_B11','Clear_B12']\n",
    "\n",
    "#Find X and y\n",
    "X=df[X_cols]\n",
    "y=df[y_cols]\n",
    "\n",
    "#Separate testdata from rest for 80/10/10 Train/Val/Test split\n",
    "X_trainval, X_test, y_trainval, y_test=train_test_split(X,y,test_size=0.1,random_state=313)\n",
    "\n",
    "#Add noise to X_test, 0 mean with stdev equal to 3% of mean of each feature\n",
    "np.random.seed(313)\n",
    "X_test = X_test + np.random.randn(np.shape(X_test)[0],np.shape(X_test)[1]) * np.mean(X.to_numpy(),axis=0)*0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up which quantiles to estimate, and find index of estimator (q=0.5)\n",
    "quantiles=np.array([0.1,0.5,0.9])\n",
    "est= np.where(quantiles==0.5)[0].item()\n",
    "\n",
    "#Set up algorithm parameters for both cases\n",
    "val_size=0.1\n",
    "num_models=5 #Set number of models in ensemble\n",
    "batch_size=500\n",
    "nepochs=1000\n",
    "lr=0.003\n",
    "noise_ratio = 0.03\n",
    "early_break=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose if to save models and data, if so set paths\n",
    "save_load=True\n",
    "if save_load:\n",
    "    test_name_1 = \"Angles_as_input-with\"\n",
    "    main_filepath_1 = 'pytorch_models/'+test_name_1\n",
    "    test_name_2 = \"Angles_as_input-without\"\n",
    "    main_filepath_2 = 'pytorch_models/'+test_name_2\n",
    "    test_name_3 = \"Angles_as_input-Sun_Zen\"\n",
    "    main_filepath_3 = 'pytorch_models/'+test_name_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: Angles included as input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 351.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.0786493] Validation loss [2.1393113]\n",
      "Epoch 462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 353.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.0856504] Validation loss [2.1265461]\n",
      "Epoch 463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 347.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.0798266] Validation loss [2.1322474]\n",
      "---No improvement in 100 epochs, broke early---\n",
      "Best model out of total max epochs found at epoch 363\n",
      "With validation loss: 2.110596179962158\n"
     ]
    }
   ],
   "source": [
    "#Set up NN structure\n",
    "no_nodes = 100\n",
    "\n",
    "sequence= lambda: nn.Sequential(\n",
    "    nn.Linear(len(X_cols),no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes, len(quantiles)*len(y_cols)) #Output dimesion is number of quantiles times number of target variables\n",
    ")\n",
    "\n",
    "#Initalize models\n",
    "models = [QuantileNetwork(quantiles=quantiles) for _ in range(num_models)]\n",
    "\n",
    "#Train models\n",
    "for i,model in enumerate(models):\n",
    "    #Find new train/val splits for each model for robustness\n",
    "    validation_indices=np.array(random.sample(range(len(X_trainval['Cloud_B02'])), int(len(X['Cloud_B02'])*val_size)))\n",
    "    train_indices=[i for i in range(len(X_trainval['Cloud_B02'])) if np.any(validation_indices==i)==False]  \n",
    "    #Fit model\n",
    "    model.fit(X_trainval.to_numpy(),y_trainval.to_numpy(), \n",
    "            train_indices=train_indices, \n",
    "            validation_indices=validation_indices, \n",
    "            batch_size=batch_size,\n",
    "            nepochs=nepochs,\n",
    "            sequence=sequence(),\n",
    "            lr=lr,\n",
    "            noise_ratio=noise_ratio,\n",
    "            early_break=early_break)\n",
    "    \n",
    "    #Save models if wanted\n",
    "    if save_load:\n",
    "        filepath=main_filepath_1+'/model'+str(i)\n",
    "        os.makedirs(filepath,exist_ok=True)\n",
    "        torch.save(model,filepath+'/model_file')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54489/4082352328.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  with_model_metrics=pd.concat([with_model_metrics,tmp_metrics])\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "if save_load:\n",
    "    base_path = main_filepath_1 + '/'\n",
    "    model_paths = ['model0/model_file','model1/model_file','model2/model_file','model3/model_file','model4/model_file']\n",
    "    models = [torch.load(base_path+model_paths[i]) for i in range(len(model_paths))]\n",
    "\n",
    "#Manually set quantiles\n",
    "quantiles = np.array([0.1,0.5,0.9])\n",
    "est = np.where(quantiles==0.5)[0].item()\n",
    "\n",
    "#Initialize dataframe for error metrics and array for ensemble predictions\n",
    "with_model_metrics=pd.DataFrame(columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "preds_total=[]\n",
    "#Make predictions and evaluate\n",
    "for i,model in enumerate(models):\n",
    "    preds = model.predict(X_test.to_numpy())\n",
    "    #Keep track of ensemble prediction\n",
    "    if i==0:\n",
    "        preds_total=preds\n",
    "    else:\n",
    "        preds_total=preds_total+preds\n",
    "\n",
    "    #Find errors\n",
    "    mse=mean_squared_error(y_test.to_numpy(),preds[:,:,est])\n",
    "    psnr=QuantileNetwork.PSNR(y_test.to_numpy(),preds[:,:,est])\n",
    "    r2=r2_score(y_test.to_numpy(),preds[:,:,est])\n",
    "    mean_quantile=QuantileNetwork.mean_marginal_loss(y_test.to_numpy(),preds,quantiles)\n",
    "    #Add to dataframe\n",
    "    tmp_metrics=pd.DataFrame(data=[[False,i,mse,psnr,r2,mean_quantile]],columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "    with_model_metrics=pd.concat([with_model_metrics,tmp_metrics])\n",
    "\n",
    "\n",
    "#Now do the same for ensemble predictions\n",
    "preds_total=preds_total/len(models)\n",
    "\n",
    "mse=mean_squared_error(y_test.to_numpy(),preds_total[:,:,est])\n",
    "psnr=QuantileNetwork.PSNR(y_test.to_numpy(),preds_total[:,:,est])\n",
    "r2=r2_score(y_test.to_numpy(),preds_total[:,:,est])\n",
    "mean_quantile=QuantileNetwork.mean_marginal_loss(y_test.to_numpy(),preds_total,quantiles)\n",
    "\n",
    "tmp_metrics=pd.DataFrame(data=[[True,np.nan,mse,psnr,r2,mean_quantile]],columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "with_model_metrics=pd.concat([with_model_metrics,tmp_metrics])\n",
    "\n",
    "#Save metrics if we want to\n",
    "if save_load:\n",
    "    with_model_metrics=with_model_metrics.reset_index(drop=True)\n",
    "    with_model_metrics.to_csv(main_filepath_1+'/model_metrics.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: Exclude angles as input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 353.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.34224] Validation loss [2.3648329]\n",
      "Epoch 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 359.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.3388112] Validation loss [2.3444405]\n",
      "Epoch 323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 391.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.3423407] Validation loss [2.377283]\n",
      "Epoch 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 386.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.340853] Validation loss [2.3412757]\n",
      "Epoch 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 361.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.3477728] Validation loss [2.3406794]\n",
      "Epoch 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 362.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.345074] Validation loss [2.3552485]\n",
      "Epoch 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 362.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.3470492] Validation loss [2.3756452]\n",
      "Epoch 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 357.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.3529305] Validation loss [2.3650446]\n",
      "Epoch 329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 353.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.344084] Validation loss [2.363338]\n",
      "---No improvement in 100 epochs, broke early---\n",
      "Best model out of total max epochs found at epoch 229\n",
      "With validation loss: 2.3265156745910645\n"
     ]
    }
   ],
   "source": [
    "#Set up NN structure, 3 less in input dim\n",
    "no_nodes = 100\n",
    "\n",
    "sequence= lambda: nn.Sequential(\n",
    "    nn.Linear(len(X_cols)-3,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes, len(quantiles)*len(y_cols)) #Output dimesion is number of quantiles times number of target variables (one)\n",
    ")\n",
    "\n",
    "#Initalize models\n",
    "models = [QuantileNetwork(quantiles=quantiles) for _ in range(num_models)]\n",
    "\n",
    "#Train models\n",
    "for i,model in enumerate(models):\n",
    "    #Find new train/val splits for each model for robustness\n",
    "    validation_indices=np.array(random.sample(range(len(X_trainval['Cloud_B02'])), int(len(X['Cloud_B02'])*val_size)))\n",
    "    train_indices=[i for i in range(len(X_trainval['Cloud_B02'])) if np.any(validation_indices==i)==False]  \n",
    "    #Fit model with y being only band 11\n",
    "    model.fit(X_trainval.to_numpy()[:,:12],y_trainval.to_numpy(), \n",
    "            train_indices=train_indices, \n",
    "            validation_indices=validation_indices, \n",
    "            batch_size=batch_size,\n",
    "            nepochs=nepochs,\n",
    "            sequence=sequence(),\n",
    "            lr=lr,\n",
    "            noise_ratio=noise_ratio,\n",
    "            early_break=early_break)\n",
    "    \n",
    "    #Save models if wanted\n",
    "    if save_load:\n",
    "        filepath=main_filepath_2+'/model'+str(i)\n",
    "        os.makedirs(filepath,exist_ok=True)\n",
    "        torch.save(model,filepath+'/model_file')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147790/3042881213.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  without_model_metrics=pd.concat([without_model_metrics,tmp_metrics])\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "if save_load:\n",
    "    base_path = main_filepath_2 + '/'\n",
    "    model_paths = ['model0/model_file','model1/model_file','model2/model_file','model3/model_file','model4/model_file']\n",
    "    models = [torch.load(base_path+model_paths[i]) for i in range(len(model_paths))]\n",
    "\n",
    "#Manually set quantiles\n",
    "quantiles = np.array([0.1,0.5,0.9])\n",
    "est = np.where(quantiles==0.5)[0].item()\n",
    "\n",
    "#Initialize dataframe for error metrics and array for ensemble predictions\n",
    "without_model_metrics=pd.DataFrame(columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "preds_total=[]\n",
    "#Make predictions and evaluate\n",
    "for i,model in enumerate(models):\n",
    "    preds = model.predict(X_test.to_numpy()[:,:12])\n",
    "    #Keep track of ensemble prediction\n",
    "    if i==0:\n",
    "        preds_total=preds\n",
    "    else:\n",
    "        preds_total=preds_total+preds\n",
    "\n",
    "    #Find errors\n",
    "    mse=mean_squared_error(y_test.to_numpy(),preds[:,:,est])\n",
    "    psnr=QuantileNetwork.PSNR(y_test.to_numpy(),preds[:,:,est])\n",
    "    r2=r2_score(y_test.to_numpy(),preds[:,:,est])\n",
    "    mean_quantile=QuantileNetwork.mean_marginal_loss(y_test.to_numpy(),preds,quantiles)\n",
    "    #Add to dataframe\n",
    "    tmp_metrics=pd.DataFrame(data=[[False,i,mse,psnr,r2,mean_quantile]],columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "    without_model_metrics=pd.concat([without_model_metrics,tmp_metrics])\n",
    "\n",
    "\n",
    "#Now do the same for ensemble predictions\n",
    "preds_total=preds_total/len(models)\n",
    "\n",
    "mse=mean_squared_error(y_test.to_numpy(),preds_total[:,:,est])\n",
    "psnr=QuantileNetwork.PSNR(y_test.to_numpy(),preds_total[:,:,est])\n",
    "r2=r2_score(y_test.to_numpy(),preds_total[:,:,est])\n",
    "mean_quantile=QuantileNetwork.mean_marginal_loss(y_test.to_numpy(),preds_total,quantiles)\n",
    "\n",
    "tmp_metrics=pd.DataFrame(data=[[True,np.nan,mse,psnr,r2,mean_quantile]],columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "without_model_metrics=pd.concat([without_model_metrics,tmp_metrics])\n",
    "\n",
    "#Save metrics if we want to\n",
    "if save_load:\n",
    "    without_model_metrics=without_model_metrics.reset_index(drop=True)\n",
    "    without_model_metrics.to_csv(main_filepath_2+'/model_metrics.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3: Only include Sun Zenith Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:00<00:00, 341.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.1007407] Validation loss [2.0930197]\n",
      "Epoch 422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 100%|██████████| 320/320 [00:01<00:00, 292.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.106531] Validation loss [2.1252537]\n",
      "Epoch 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:01<00:00, 292.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.1000469] Validation loss [2.0998294]\n",
      "Epoch 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 100%|██████████| 320/320 [00:01<00:00, 292.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [2.0997488] Validation loss [2.0832274]\n",
      "---No improvement in 100 epochs, broke early---\n",
      "Best model out of total max epochs found at epoch 324\n",
      "With validation loss: 2.071849822998047\n"
     ]
    }
   ],
   "source": [
    "#Set up NN structure, 2 less in input dim\n",
    "no_nodes = 100\n",
    "\n",
    "sequence= lambda: nn.Sequential(\n",
    "    nn.Linear(len(X_cols)-2,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes,no_nodes),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(no_nodes, len(quantiles)*len(y_cols)) #Output dimesion is number of quantiles times number of target variables (one)\n",
    ")\n",
    "\n",
    "#Initalize models\n",
    "models = [QuantileNetwork(quantiles=quantiles) for _ in range(num_models)]\n",
    "\n",
    "#Train models\n",
    "for i,model in enumerate(models):\n",
    "    #Find new train/val splits for each model for robustness\n",
    "    validation_indices=np.array(random.sample(range(len(X_trainval['Cloud_B02'])), int(len(X['Cloud_B02'])*val_size)))\n",
    "    train_indices=[i for i in range(len(X_trainval['Cloud_B02'])) if np.any(validation_indices==i)==False]  \n",
    "    #Fit model with y being only band 11\n",
    "    model.fit(X_trainval.to_numpy()[:,[0,1,2,3,4,5,6,7,8,9,10,11,13]],y_trainval.to_numpy(), \n",
    "            train_indices=train_indices, \n",
    "            validation_indices=validation_indices, \n",
    "            batch_size=batch_size,\n",
    "            nepochs=nepochs,\n",
    "            sequence=sequence(),\n",
    "            lr=lr,\n",
    "            noise_ratio=noise_ratio,\n",
    "            early_break=early_break)\n",
    "    \n",
    "    #Save models if wanted\n",
    "    if save_load:\n",
    "        filepath=main_filepath_3+'/model'+str(i)\n",
    "        os.makedirs(filepath,exist_ok=True)\n",
    "        torch.save(model,filepath+'/model_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197993/388508426.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sunzen_model_metrics=pd.concat([sunzen_model_metrics,tmp_metrics])\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "if save_load:\n",
    "    base_path = main_filepath_3 + '/'\n",
    "    model_paths = ['model0/model_file','model1/model_file','model2/model_file','model3/model_file','model4/model_file']\n",
    "    models = [torch.load(base_path+model_paths[i]) for i in range(len(model_paths))]\n",
    "\n",
    "#Manually set quantiles\n",
    "quantiles = np.array([0.1,0.5,0.9])\n",
    "est = np.where(quantiles==0.5)[0].item()\n",
    "\n",
    "#Initialize dataframe for error metrics and array for ensemble predictions\n",
    "sunzen_model_metrics=pd.DataFrame(columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "preds_total=[]\n",
    "#Make predictions and evaluate\n",
    "for i,model in enumerate(models):\n",
    "    preds = model.predict(X_test.to_numpy()[:,[0,1,2,3,4,5,6,7,8,9,10,11,13]])\n",
    "    #Keep track of ensemble prediction\n",
    "    if i==0:\n",
    "        preds_total=preds\n",
    "    else:\n",
    "        preds_total=preds_total+preds\n",
    "\n",
    "    #Find errors\n",
    "    mse=mean_squared_error(y_test.to_numpy(),preds[:,:,est])\n",
    "    psnr=QuantileNetwork.PSNR(y_test.to_numpy(),preds[:,:,est])\n",
    "    r2=r2_score(y_test.to_numpy(),preds[:,:,est])\n",
    "    mean_quantile=QuantileNetwork.mean_marginal_loss(y_test.to_numpy(),preds,quantiles)\n",
    "    #Add to dataframe\n",
    "    tmp_metrics=pd.DataFrame(data=[[False,i,mse,psnr,r2,mean_quantile]],columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "    sunzen_model_metrics=pd.concat([sunzen_model_metrics,tmp_metrics])\n",
    "\n",
    "\n",
    "#Now do the same for ensemble predictions\n",
    "preds_total=preds_total/len(models)\n",
    "\n",
    "mse=mean_squared_error(y_test.to_numpy(),preds_total[:,:,est])\n",
    "psnr=QuantileNetwork.PSNR(y_test.to_numpy(),preds_total[:,:,est])\n",
    "r2=r2_score(y_test.to_numpy(),preds_total[:,:,est])\n",
    "mean_quantile=QuantileNetwork.mean_marginal_loss(y_test.to_numpy(),preds_total,quantiles)\n",
    "\n",
    "tmp_metrics=pd.DataFrame(data=[[True,np.nan,mse,psnr,r2,mean_quantile]],columns=['Ensemble_mean','Ensemble_index','MSE','PSNR','R2_score','Mean_Quantile_Loss'])\n",
    "sunzen_model_metrics=pd.concat([sunzen_model_metrics,tmp_metrics])\n",
    "\n",
    "#Save metrics if we want to\n",
    "if save_load:\n",
    "    sunzen_model_metrics=sunzen_model_metrics.reset_index(drop=True)\n",
    "    sunzen_model_metrics.to_csv(main_filepath_3+'/model_metrics.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ensemble_mean</th>\n",
       "      <th>Ensemble_index</th>\n",
       "      <th>MSE</th>\n",
       "      <th>PSNR</th>\n",
       "      <th>R2_score</th>\n",
       "      <th>Mean_Quantile_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>21.633462</td>\n",
       "      <td>0.843384</td>\n",
       "      <td>0.438998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>21.659928</td>\n",
       "      <td>0.843847</td>\n",
       "      <td>0.435985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>21.606712</td>\n",
       "      <td>0.842696</td>\n",
       "      <td>0.436534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>21.667865</td>\n",
       "      <td>0.844638</td>\n",
       "      <td>0.438576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>21.573897</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>0.438437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>21.906343</td>\n",
       "      <td>0.852975</td>\n",
       "      <td>0.418722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ensemble_mean Ensemble_index       MSE       PSNR  R2_score  \\\n",
       "0         False              0  0.006911  21.633462  0.843384   \n",
       "1         False              1  0.006869  21.659928  0.843847   \n",
       "2         False              2  0.006953  21.606712  0.842696   \n",
       "3         False              3  0.006856  21.667865  0.844638   \n",
       "4         False              4  0.007006  21.573897  0.841085   \n",
       "5          True            NaN  0.006490  21.906343  0.852975   \n",
       "\n",
       "   Mean_Quantile_Loss  \n",
       "0            0.438998  \n",
       "1            0.435985  \n",
       "2            0.436534  \n",
       "3            0.438576  \n",
       "4            0.438437  \n",
       "5            0.418722  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results with angles\n",
    "if save_load:\n",
    "    file_name = main_filepath_1 + '/model_metrics.csv'\n",
    "    with_model_metrics=pd.read_csv(file_name)\n",
    "    \n",
    "with_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ensemble_mean</th>\n",
       "      <th>Ensemble_index</th>\n",
       "      <th>MSE</th>\n",
       "      <th>PSNR</th>\n",
       "      <th>R2_score</th>\n",
       "      <th>Mean_Quantile_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>20.868457</td>\n",
       "      <td>0.809809</td>\n",
       "      <td>0.487105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>20.898331</td>\n",
       "      <td>0.810577</td>\n",
       "      <td>0.486688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>20.887634</td>\n",
       "      <td>0.809990</td>\n",
       "      <td>0.485302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>20.898052</td>\n",
       "      <td>0.810647</td>\n",
       "      <td>0.483422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>20.875906</td>\n",
       "      <td>0.809692</td>\n",
       "      <td>0.483046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>21.144368</td>\n",
       "      <td>0.821311</td>\n",
       "      <td>0.465506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>21.899997</td>\n",
       "      <td>0.852879</td>\n",
       "      <td>0.421465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ensemble_mean Ensemble_index       MSE       PSNR  R2_score  \\\n",
       "0         False              0  0.008242  20.868457  0.809809   \n",
       "1         False              1  0.008185  20.898331  0.810577   \n",
       "2         False              2  0.008206  20.887634  0.809990   \n",
       "3         False              3  0.008186  20.898052  0.810647   \n",
       "4         False              4  0.008228  20.875906  0.809692   \n",
       "5          True            NaN  0.007735  21.144368  0.821311   \n",
       "0          True            NaN  0.006499  21.899997  0.852879   \n",
       "\n",
       "   Mean_Quantile_Loss  \n",
       "0            0.487105  \n",
       "1            0.486688  \n",
       "2            0.485302  \n",
       "3            0.483422  \n",
       "4            0.483046  \n",
       "5            0.465506  \n",
       "0            0.421465  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results without angles\n",
    "if save_load:\n",
    "    file_name = main_filepath_2 + '/model_metrics.csv'\n",
    "    without_model_metrics=pd.read_csv(file_name)\n",
    "    \n",
    "without_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ensemble_mean</th>\n",
       "      <th>Ensemble_index</th>\n",
       "      <th>MSE</th>\n",
       "      <th>PSNR</th>\n",
       "      <th>R2_score</th>\n",
       "      <th>Mean_Quantile_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>21.695823</td>\n",
       "      <td>0.845446</td>\n",
       "      <td>0.438479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>21.595004</td>\n",
       "      <td>0.841873</td>\n",
       "      <td>0.437508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006858</td>\n",
       "      <td>21.666677</td>\n",
       "      <td>0.844476</td>\n",
       "      <td>0.439852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>21.708013</td>\n",
       "      <td>0.846127</td>\n",
       "      <td>0.438455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>21.620881</td>\n",
       "      <td>0.843294</td>\n",
       "      <td>0.439390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>21.919294</td>\n",
       "      <td>0.853453</td>\n",
       "      <td>0.419976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ensemble_mean Ensemble_index       MSE       PSNR  R2_score  \\\n",
       "0         False              0  0.006812  21.695823  0.845446   \n",
       "1         False              1  0.006972  21.595004  0.841873   \n",
       "2         False              2  0.006858  21.666677  0.844476   \n",
       "3         False              3  0.006793  21.708013  0.846127   \n",
       "4         False              4  0.006931  21.620881  0.843294   \n",
       "5          True            NaN  0.006471  21.919294  0.853453   \n",
       "\n",
       "   Mean_Quantile_Loss  \n",
       "0            0.438479  \n",
       "1            0.437508  \n",
       "2            0.439852  \n",
       "3            0.438455  \n",
       "4            0.439390  \n",
       "5            0.419976  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results with sun zenith angle\n",
    "if save_load:\n",
    "    file_name = main_filepath_3 + '/model_metrics.csv'\n",
    "    sunzen_model_metrics=pd.read_csv(file_name)\n",
    "sunzen_model_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
