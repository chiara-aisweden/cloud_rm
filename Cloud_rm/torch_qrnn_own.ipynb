{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "import functions.parse_data as parse\n",
    "import functions.handy_functions as hf\n",
    "\n",
    "from multivariate_quantile_regression.network_model import QuantileNetwork\n",
    "from py_torch_qrnn_adapt.funcs import Sanity, Scenario1, Scenario2, Scenario3, Scenario4, Scenario5,\\\n",
    "                  MultivariateScenario1, MultivariateScenario2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_labels= ['Cloud_B02','Cloud_B03','Cloud_B04','Cloud_B05','Cloud_B06','Cloud_B07',\n",
    "                 'Cloud_B08','Cloud_B09','Cloud_B10','Cloud_B11','Cloud_B12','Cloud_B13']\n",
    "\n",
    "data_water=parse.parse('cloudrm_water.dat')\n",
    "data_clear=parse.parse('cloudrm_clear.dat')\n",
    "data_ice=parse.parse('cloudrm_ice.dat')\n",
    "data_mixed=parse.parse('cloudrm_mixed.dat')\n",
    "\n",
    "#Concatinate all datasets\n",
    "data_all=pd.concat([data_water, data_clear, data_ice, data_mixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise standard deviation for Cloud_B02: 0.00335001428051948\n",
      "Noise standard deviation for Cloud_B03: 0.002912530185416667\n",
      "Noise standard deviation for Cloud_B04: 0.004058081082042254\n",
      "Noise standard deviation for Cloud_B05: 0.0046524891611111115\n",
      "Noise standard deviation for Cloud_B06: 0.007455351321348316\n",
      "Noise standard deviation for Cloud_B07: 0.008871707484285717\n",
      "Noise standard deviation for Cloud_B08: 0.04489677938000001\n",
      "Noise standard deviation for Cloud_B09: 0.005688141120114942\n",
      "Noise standard deviation for Cloud_B10: 0.003909328971491229\n",
      "Noise standard deviation for Cloud_B11: 0.0014014724139999996\n",
      "Noise standard deviation for Cloud_B12: 0.005030040539999999\n",
      "Noise standard deviation for Cloud_B13: 0.004041267081999999\n"
     ]
    }
   ],
   "source": [
    "channel_labels= ['Cloud_B02','Cloud_B03','Cloud_B04','Cloud_B05','Cloud_B06','Cloud_B07',\n",
    "                 'Cloud_B08','Cloud_B09','Cloud_B10','Cloud_B11','Cloud_B12','Cloud_B13']\n",
    "\n",
    "data_water=parse.parse('cloudrm_water.dat')\n",
    "data_clear=parse.parse('cloudrm_clear.dat')\n",
    "data_ice=parse.parse('cloudrm_ice.dat')\n",
    "data_mixed=parse.parse('cloudrm_mixed.dat')\n",
    "\n",
    "#Concatinate all datasets\n",
    "data_all=pd.concat([data_water, data_clear, data_ice, data_mixed])\n",
    "data_all=data_all.drop(columns=['Surface_Desc','Cloud_B01','Clear_B01'])\n",
    "data_all=hf.add_MSI_noise(data_all,channel_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train test validation split##\n",
    "X_labels= ['Cloud_B02','Cloud_B03','Cloud_B04','Cloud_B05','Cloud_B06',\n",
    "           'Cloud_B07','Cloud_B08','Cloud_B09','Cloud_B10','Cloud_B11','Cloud_B12','Cloud_B13',\n",
    "           'Sat_Zenith_Angle','Sun_Zenith_Angle','Azimuth_Diff_Angle']\n",
    "\n",
    "#Leave out 'GOT', 'Water_Vapor'\n",
    "#Band 1 no go.\n",
    "y_labels=['Clear_B02','Clear_B03','Clear_B04','Clear_B05','Clear_B06',\n",
    "           'Clear_B07','Clear_B08','Clear_B09','Clear_B10','Clear_B11','Clear_B12','Clear_B13']\n",
    "\n",
    "df=hf.normalise_input_df(data_all,X_labels)\n",
    "df=hf.add_noise(df,X_labels,sigma=0.001)\n",
    "num_epochs=2\n",
    "batch_size=100\n",
    "\n",
    "##Split data##\n",
    "X=df[X_labels]\n",
    "y=df[y_labels]\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "val_size=0.05\n",
    "validation_indices=np.array(random.sample(range(len(X_train['Cloud_B02'])), int(len(X_train['Cloud_B02'])*val_size)))\n",
    "train_indices=[i for i in range(len(X_train['Cloud_B02'])) if np.any(validation_indices==i)==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:02, 611.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 192.5401153564453 Validation loss 34997.984375\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:03, 542.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 177.94204711914062 Validation loss 32594.345703125\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:03, 540.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 166.33506774902344 Validation loss 30607.955078125\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 1805it [00:03, 513.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 156.8061065673828 Validation loss 28993.8984375\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 1805it [00:03, 505.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 149.3293914794922 Validation loss 27770.93359375\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:03, 532.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 143.88873291015625 Validation loss 26920.9921875\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:03, 500.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 140.3376007080078 Validation loss 26387.443359375\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch number: 1805it [00:03, 495.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 138.048828125 Validation loss 26014.38671875\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:03, 530.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 136.27542114257812 Validation loss 25699.357421875\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch number: 1805it [00:03, 506.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 134.6904754638672 Validation loss 25408.068359375\n"
     ]
    }
   ],
   "source": [
    "quantiles=np.array([0.1,0.5,0.9])\n",
    "batch_size=100\n",
    "nepochs=10\n",
    "\n",
    "model=QuantileNetwork(quantiles=quantiles)\n",
    "model.fit(X_train.to_numpy(),y_train.to_numpy(),train_indices,validation_indices,batch_size=batch_size,nepochs=nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07781808355937571"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test.to_numpy(),preds[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(y_true,y_pred):\n",
    "    mse = mean_squared_error(y_true,y_pred)\n",
    "    maxval = np.amax(y_true)\n",
    "    PSNR = 10*np.log10((maxval)**2/mse)\n",
    "    \n",
    "    return PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.426843537295962"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PSNR(y_test,preds[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np=y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_outrate(y_test,preds):\n",
    "    outcount = 0\n",
    "    for i in range(np.shape(y_test_np)[0]):\n",
    "        for j in range(np.shape(y_test_np)[1]):\n",
    "            if y_test_np[i,j] < preds[i,j,0] or y_test_np[i,j] > preds[i,j,2]:\n",
    "                outcount = outcount +1\n",
    "\n",
    "    outrate = outcount/np.size(y_test)\n",
    "    return outrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026941666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcount = 0\n",
    "for i in range(np.shape(y_test_np)[0]):\n",
    "    for j in range(np.shape(y_test_np)[1]):\n",
    "        if y_test_np[i,j] < preds[i,j,0] or y_test_np[i,j] > preds[i,j,2]:\n",
    "            outcount = outcount +1\n",
    "\n",
    "outrate = outcount/np.size(y_test)\n",
    "\n",
    "outrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
