{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add noise (not done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_snr = [128, 201, 243, 228, 74, 275, 110, 880, 838, 802, 754, 750, 910, 1087, 586, 516, 167, 57, 250,\n",
    "        None, None, None, None, None, None, 150, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "bands_netd = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "         None, None, 0.05, 0.2, 0.07, 0.07, 0.25, 0.25, None, 0.25, 0.25, 0.05, 0.25, 0.05, 0.05, 0.25, 0.25, 0.25, 0.35]\n",
    "\n",
    "\n",
    "snr_netd = zip(bands_snr, bands_netd)\n",
    "all_bands = data_ir_vis[:,1:30]\n",
    "print(all_bands[0,:])\n",
    "\n",
    "print(all_bands.shape)\n",
    "\n",
    "for band_i, (snr, netd) in enumerate(snr_netd):\n",
    "    if snr != None:\n",
    "        avg_squared_signal_pwr = np.mean(np.square(all_bands[:, band_i]))\n",
    "        noise_std = np.sqrt(avg_squared_signal_pwr / snr)\n",
    "    else:\n",
    "        noise_std = netd\n",
    "\n",
    "    all_bands[:, band_i] += np.random.normal(0, noise_std, all_bands[:, band_i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Define the binary cross-entropy loss\n",
    "    bce_loss = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Define a penalty for false negatives\n",
    "    # Adjust the value of alpha based on the importance of reducing false negatives\n",
    "    alpha = 0.5  # Example value, you can adjust as needed\n",
    "    false_negative_penalty = alpha * K.sum(y_true * K.cast(K.greater(y_pred, 0.5), 'float32'), axis=-1)\n",
    "    \n",
    "    # Add the penalty to the loss\n",
    "    total_loss = bce_loss + false_negative_penalty\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FIFTY_FIFTY:\n",
    "\n",
    "    #data_ir_vis, data_test = train_test_split(data_ir_vis, test_size=0.05, random_state=1)\n",
    "\n",
    "    low_vis = data_ir_vis[np.where(data_ir_vis[:,30] < low_vis_threshold)[0]]\n",
    "\n",
    "    #raw_data_ir = pd.read_csv('fog_dataset/' + 'fog_data_ir_clear.dat').to_numpy()\n",
    "    #raw_data_vis = pd.read_csv('fog_dataset/' + 'fog_data_vis_clear.dat').to_numpy()\n",
    "    #data_ir = np.array([row[0].split() for row in raw_data_ir[25:]])\n",
    "\n",
    "    \n",
    "    \n",
    "    #data_vis = np.array([row[0].split()[1:] for row in raw_data_vis[41:]])\n",
    "\n",
    "    #tmp_data = np.hstack([data_ir, data_vis])\n",
    "    #tmp_data = tmp_data[:,:-1] \n",
    "    #tmp_data = tmp_data.astype(np.float32)\n",
    "\n",
    "    high_vis_i = np.where(data_ir_vis[:,30] >= low_vis_threshold)[0]      #tmp_data[:,30]\n",
    "    if len(low_vis) > len(high_vis_i):\n",
    "       print('Not enough high vis data for 50/50')\n",
    "    r_high_vis_i = np.random.choice(high_vis_i, len(low_vis), replace=False)\n",
    "    fifty_fifty = np.vstack([low_vis, data_ir_vis[r_high_vis_i]])       #tmp_data[r_high_vis_i]]\n",
    "    data_ir_vis = fifty_fifty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train_label \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros_like(y_train, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      2\u001b[0m y_val_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_val, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_test_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_test, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_label = np.zeros_like(y_train, dtype=int)\n",
    "y_val_label = np.zeros_like(y_val, dtype=int)\n",
    "y_test_label = np.zeros_like(y_test, dtype=int)\n",
    "\n",
    "for i in np.where(target_scaler.inverse_transform(y_train) < low_vis_threshold)[0]:\n",
    "   y_train_label[i] = 1\n",
    "\n",
    "for i in np.where(target_scaler.inverse_transform(y_val) < low_vis_threshold)[0]:\n",
    "   y_val_label[i] = 1\n",
    "\n",
    "for i in np.where(target_scaler.inverse_transform(y_test) < low_vis_threshold)[0]:\n",
    "   y_test_label[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "USE_MIXED = False\n",
    "USE_PRESSURE_TEMP = False\n",
    "noise_std_dev = 1e-3\n",
    "\n",
    "\n",
    "if USE_MIXED:\n",
    "    filenames_vis = ['fog_data_vis_clear.dat', 'fog_data_vis_ice.dat', 'fog_data_vis_mixed.dat', 'fog_data_vis_water.dat']\n",
    "    filenames_ir = ['fog_data_ir_clear.dat', 'fog_data_ir_ice.dat', 'fog_data_ir_mixed.dat', 'fog_data_ir_water.dat']\n",
    "else:\n",
    "    filenames_vis = ['fog_data_vis_clear.dat', 'fog_data_vis_ice.dat', 'fog_data_vis_water.dat']\n",
    "    filenames_ir = ['fog_data_ir_clear.dat', 'fog_data_ir_ice.dat', 'fog_data_ir_water.dat']\n",
    "\n",
    "target_i = 30 # Visibility\n",
    "if USE_PRESSURE_TEMP:\n",
    "    features_i = np.concatenate((np.arange(4, 17), np.arange(20, 30), np.arange(31,36))) # 13 ir bands, 10 vis bands, 3 angles, pressure, temp\n",
    "else:\n",
    "    features_i = np.concatenate((np.arange(4, 17), np.arange(20, 30), np.arange(31,34)))\n",
    "\n",
    "\n",
    "\n",
    "data_ir_vis = []\n",
    "\n",
    "for file_ir, file_vis in zip(filenames_ir, filenames_vis):\n",
    "  raw_data_ir = pd.read_csv('fog_dataset/' + file_ir).to_numpy()\n",
    "  raw_data_vis = pd.read_csv('fog_dataset/' + file_vis).to_numpy()\n",
    "\n",
    "  data_ir = np.array([row[0].split() for row in raw_data_ir[25:]])\n",
    "  data_vis = np.array([row[0].split()[1:] for row in raw_data_vis[41:]])\n",
    "\n",
    "  if len(data_ir_vis) == 0:\n",
    "    data_ir_vis = np.hstack([data_ir, data_vis])\n",
    "  else:\n",
    "    data_ir_vis = np.vstack([data_ir_vis, np.hstack([data_ir, data_vis])])\n",
    "\n",
    "\n",
    "# Remove surface description to convert to float\n",
    "data_ir_vis = data_ir_vis[:,:-1] \n",
    "data_ir_vis = data_ir_vis.astype(np.double)\n",
    "\n",
    "nan_i = np.where(np.isnan(data_ir_vis))[0]\n",
    "data_ir_vis = np.delete(data_ir_vis, nan_i, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "X = np.zeros((data_ir_vis.shape[0], len(features_i)))\n",
    "y = np.zeros(data_ir_vis.shape[0])\n",
    "\n",
    "for i in range(len(data_ir_vis)):\n",
    "  X[i] = data_ir_vis[i, features_i]\n",
    "  y[i] = data_ir_vis[i,target_i]\n",
    "\n",
    "# 19 Data points contains nan. Not sure why. Band 32 culprit, 8th feature\n",
    "nan_i = np.where(np.isnan(X))[0]\n",
    "print(f'nani: {nan_i}')\n",
    "X = np.delete(X, nan_i, axis=0)\n",
    "y = np.delete(y, nan_i, axis=0)\n",
    "\n",
    "\n",
    "# Scaling features\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_noisy = X_scaled + np.random.normal(0, noise_std_dev, X_scaled.shape)\n",
    "\n",
    "# Scaling target\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# First split: Separate out a test set (5% of the original dataset)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled_noisy, y_scaled, test_size=(1/20), random_state=16)\n",
    "\n",
    "# Second split: Split the remaining data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=(1/19), random_state=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
