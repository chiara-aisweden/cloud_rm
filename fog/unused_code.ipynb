{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add noise (not done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_snr = [128, 201, 243, 228, 74, 275, 110, 880, 838, 802, 754, 750, 910, 1087, 586, 516, 167, 57, 250,\n",
    "        None, None, None, None, None, None, 150, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "bands_netd = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "         None, None, 0.05, 0.2, 0.07, 0.07, 0.25, 0.25, None, 0.25, 0.25, 0.05, 0.25, 0.05, 0.05, 0.25, 0.25, 0.25, 0.35]\n",
    "\n",
    "\n",
    "snr_netd = zip(bands_snr, bands_netd)\n",
    "all_bands = data_ir_vis[:,1:30]\n",
    "print(all_bands[0,:])\n",
    "\n",
    "print(all_bands.shape)\n",
    "\n",
    "for band_i, (snr, netd) in enumerate(snr_netd):\n",
    "    if snr != None:\n",
    "        avg_squared_signal_pwr = np.mean(np.square(all_bands[:, band_i]))\n",
    "        noise_std = np.sqrt(avg_squared_signal_pwr / snr)\n",
    "    else:\n",
    "        noise_std = netd\n",
    "\n",
    "    all_bands[:, band_i] += np.random.normal(0, noise_std, all_bands[:, band_i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Define the binary cross-entropy loss\n",
    "    bce_loss = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Define a penalty for false negatives\n",
    "    # Adjust the value of alpha based on the importance of reducing false negatives\n",
    "    alpha = 0.5  # Example value, you can adjust as needed\n",
    "    false_negative_penalty = alpha * K.sum(y_true * K.cast(K.greater(y_pred, 0.5), 'float32'), axis=-1)\n",
    "    \n",
    "    # Add the penalty to the loss\n",
    "    total_loss = bce_loss + false_negative_penalty\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FIFTY_FIFTY:\n",
    "\n",
    "    #data_ir_vis, data_test = train_test_split(data_ir_vis, test_size=0.05, random_state=1)\n",
    "\n",
    "    low_vis = data_ir_vis[np.where(data_ir_vis[:,30] < low_vis_threshold)[0]]\n",
    "\n",
    "    #raw_data_ir = pd.read_csv('fog_dataset/' + 'fog_data_ir_clear.dat').to_numpy()\n",
    "    #raw_data_vis = pd.read_csv('fog_dataset/' + 'fog_data_vis_clear.dat').to_numpy()\n",
    "    #data_ir = np.array([row[0].split() for row in raw_data_ir[25:]])\n",
    "\n",
    "    \n",
    "    \n",
    "    #data_vis = np.array([row[0].split()[1:] for row in raw_data_vis[41:]])\n",
    "\n",
    "    #tmp_data = np.hstack([data_ir, data_vis])\n",
    "    #tmp_data = tmp_data[:,:-1] \n",
    "    #tmp_data = tmp_data.astype(np.float32)\n",
    "\n",
    "    high_vis_i = np.where(data_ir_vis[:,30] >= low_vis_threshold)[0]      #tmp_data[:,30]\n",
    "    if len(low_vis) > len(high_vis_i):\n",
    "       print('Not enough high vis data for 50/50')\n",
    "    r_high_vis_i = np.random.choice(high_vis_i, len(low_vis), replace=False)\n",
    "    fifty_fifty = np.vstack([low_vis, data_ir_vis[r_high_vis_i]])       #tmp_data[r_high_vis_i]]\n",
    "    data_ir_vis = fifty_fifty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train_label \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros_like(y_train, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      2\u001b[0m y_val_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_val, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_test_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_test, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_label = np.zeros_like(y_train, dtype=int)\n",
    "y_val_label = np.zeros_like(y_val, dtype=int)\n",
    "y_test_label = np.zeros_like(y_test, dtype=int)\n",
    "\n",
    "for i in np.where(target_scaler.inverse_transform(y_train) < low_vis_threshold)[0]:\n",
    "   y_train_label[i] = 1\n",
    "\n",
    "for i in np.where(target_scaler.inverse_transform(y_val) < low_vis_threshold)[0]:\n",
    "   y_val_label[i] = 1\n",
    "\n",
    "for i in np.where(target_scaler.inverse_transform(y_test) < low_vis_threshold)[0]:\n",
    "   y_test_label[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "USE_MIXED = False\n",
    "USE_PRESSURE_TEMP = False\n",
    "noise_std_dev = 1e-3\n",
    "\n",
    "\n",
    "if USE_MIXED:\n",
    "    filenames_vis = ['fog_data_vis_clear.dat', 'fog_data_vis_ice.dat', 'fog_data_vis_mixed.dat', 'fog_data_vis_water.dat']\n",
    "    filenames_ir = ['fog_data_ir_clear.dat', 'fog_data_ir_ice.dat', 'fog_data_ir_mixed.dat', 'fog_data_ir_water.dat']\n",
    "else:\n",
    "    filenames_vis = ['fog_data_vis_clear.dat', 'fog_data_vis_ice.dat', 'fog_data_vis_water.dat']\n",
    "    filenames_ir = ['fog_data_ir_clear.dat', 'fog_data_ir_ice.dat', 'fog_data_ir_water.dat']\n",
    "\n",
    "target_i = 30 # Visibility\n",
    "if USE_PRESSURE_TEMP:\n",
    "    features_i = np.concatenate((np.arange(4, 17), np.arange(20, 30), np.arange(31,36))) # 13 ir bands, 10 vis bands, 3 angles, pressure, temp\n",
    "else:\n",
    "    features_i = np.concatenate((np.arange(4, 17), np.arange(20, 30), np.arange(31,34)))\n",
    "\n",
    "\n",
    "\n",
    "data_ir_vis = []\n",
    "\n",
    "for file_ir, file_vis in zip(filenames_ir, filenames_vis):\n",
    "  raw_data_ir = pd.read_csv('fog_dataset/' + file_ir).to_numpy()\n",
    "  raw_data_vis = pd.read_csv('fog_dataset/' + file_vis).to_numpy()\n",
    "\n",
    "  data_ir = np.array([row[0].split() for row in raw_data_ir[25:]])\n",
    "  data_vis = np.array([row[0].split()[1:] for row in raw_data_vis[41:]])\n",
    "\n",
    "  if len(data_ir_vis) == 0:\n",
    "    data_ir_vis = np.hstack([data_ir, data_vis])\n",
    "  else:\n",
    "    data_ir_vis = np.vstack([data_ir_vis, np.hstack([data_ir, data_vis])])\n",
    "\n",
    "\n",
    "# Remove surface description to convert to float\n",
    "data_ir_vis = data_ir_vis[:,:-1] \n",
    "data_ir_vis = data_ir_vis.astype(np.double)\n",
    "\n",
    "nan_i = np.where(np.isnan(data_ir_vis))[0]\n",
    "data_ir_vis = np.delete(data_ir_vis, nan_i, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "X = np.zeros((data_ir_vis.shape[0], len(features_i)))\n",
    "y = np.zeros(data_ir_vis.shape[0])\n",
    "\n",
    "for i in range(len(data_ir_vis)):\n",
    "  X[i] = data_ir_vis[i, features_i]\n",
    "  y[i] = data_ir_vis[i,target_i]\n",
    "\n",
    "# 19 Data points contains nan. Not sure why. Band 32 culprit, 8th feature\n",
    "nan_i = np.where(np.isnan(X))[0]\n",
    "print(f'nani: {nan_i}')\n",
    "X = np.delete(X, nan_i, axis=0)\n",
    "y = np.delete(y, nan_i, axis=0)\n",
    "\n",
    "\n",
    "# Scaling features\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_noisy = X_scaled + np.random.normal(0, noise_std_dev, X_scaled.shape)\n",
    "\n",
    "# Scaling target\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# First split: Separate out a test set (5% of the original dataset)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled_noisy, y_scaled, test_size=(1/20), random_state=16)\n",
    "\n",
    "# Second split: Split the remaining data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=(1/19), random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        attn_output = self.att(inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Define your existing features_i, num_features, and other variables\n",
    "num_features = len(features_i)\n",
    "\n",
    "# Define your Transformer parameters\n",
    "num_layers = 2\n",
    "input_shape = (100, num_features)  # Example input shape: 100 timesteps, num_features features\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "output_dim = 1  # Output dimension, adjust as needed\n",
    "rate = 0.1  # Dropout rate\n",
    "\n",
    "# Build the Transformer-based model\n",
    "def build_transformer_model(num_layers, input_shape, embed_dim, num_heads, ff_dim, output_dim, rate=0.1):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerEncoder(embed_dim, num_heads, ff_dim, rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(rate)(x)\n",
    "    outputs = layers.Dense(output_dim, activation=\"sigmoid\")(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Build the Transformer-based model\n",
    "transformer_model = build_transformer_model(num_layers, input_shape, embed_dim, num_heads, ff_dim, output_dim, rate)\n",
    "\n",
    "# Concatenate the output of the Transformer model with additional features if needed\n",
    "combined_input = layers.concatenate([transformer_model.output, ff_model.output])\n",
    "\n",
    "# Add additional dense layers or any other layers as needed\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(combined_input)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the final model\n",
    "final_model = models.Model(inputs=[transformer_model.input, ff_model.input], outputs=outputs)\n",
    "\n",
    "# Compile the final model\n",
    "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # parametreleri\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # batch-layer\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TabTransformer(keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "            categories,\n",
    "            num_continuous,\n",
    "            dim,\n",
    "            dim_out,\n",
    "            depth,\n",
    "            heads,\n",
    "            attn_dropout,\n",
    "            ff_dropout,\n",
    "            mlp_hidden,\n",
    "            normalize_continuous = True):\n",
    "        \"\"\"TabTrasformer model constructor\n",
    "        Args:\n",
    "            categories (:obj:`list`): list of integers denoting the number of \n",
    "                classes for a categorical feature.\n",
    "            num_continuous (int): number of categorical features\n",
    "            dim (int): dimension of each embedding layer output, also transformer dimension\n",
    "            dim_out (int): output dimension of the model\n",
    "            depth (int): number of transformers to stack\n",
    "            heads (int): number of attention heads\n",
    "            attn_dropout (float): dropout to use in attention layer of transformers\n",
    "            ff_dropout (float): dropout to use in feed-forward layer of transformers\n",
    "            mlp_hidden (:obj:`list`): list of tuples, indicating the size of the mlp layers and\n",
    "                their activation functions\n",
    "            normalize_continuous (boolean, optional): whether the continuous features are normalized\n",
    "                before MLP layers, True by default\n",
    "        \"\"\"\n",
    "        super(TabTransformer, self).__init__()\n",
    "\n",
    "        # --> continuous inputs\n",
    "        self.normalize_continuous = normalize_continuous\n",
    "        if normalize_continuous:\n",
    "            self.continuous_normalization = layers.LayerNormalization()\n",
    "\n",
    "        # --> categorical inputs\n",
    "\n",
    "        # embedding\n",
    "        self.embedding_layers = []\n",
    "        for number_of_classes in categories:\n",
    "            self.embedding_layers.append(layers.Embedding(input_dim = number_of_classes, output_dim = dim))\n",
    "\n",
    "        # concatenation\n",
    "        self.embedded_concatenation = layers.Concatenate(axis=1)\n",
    "\n",
    "        # adding transformers\n",
    "        self.transformers = []\n",
    "        for _ in range(depth):\n",
    "            self.transformers.append(TransformerBlock(dim, heads, dim))\n",
    "        self.flatten_transformer_output = layers.Flatten()\n",
    "\n",
    "        # --> MLP\n",
    "        self.pre_mlp_concatenation = layers.Concatenate()\n",
    "\n",
    "        # mlp layers\n",
    "        self.mlp_layers = []\n",
    "        for size, activation in mlp_hidden:\n",
    "            self.mlp_layers.append(layers.Dense(size, activation=activation))\n",
    "\n",
    "        self.output_layer = layers.Dense(dim_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        continuous_inputs  = inputs[0]\n",
    "        categorical_inputs = inputs[1:]\n",
    "        \n",
    "        # --> continuous\n",
    "        if self.normalize_continuous:\n",
    "            continuous_inputs = self.continuous_normalization(continuous_inputs)\n",
    "\n",
    "        # --> categorical\n",
    "        #embedding_outputs = []\n",
    "        # for categorical_input, embedding_layer in zip(categorical_inputs, self.embedding_layers):\n",
    "        #     embedding_outputs.append(embedding_layer(categorical_input))\n",
    "        # categorical_inputs = self.embedded_concatenation(embedding_outputs)\n",
    "        embedding_outputs = tf.keras.layers.Lambda(lambda x: [layer(x_i) for x_i, layer in zip(x, self.embedding_layers)])(categorical_inputs)\n",
    "        categorical_inputs = self.embedded_concatenation(embedding_outputs)\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            categorical_inputs = transformer(categorical_inputs)\n",
    "        contextual_embedding = self.flatten_transformer_output(categorical_inputs)\n",
    "\n",
    "        # --> MLP\n",
    "        mlp_input = self.pre_mlp_concatenation([continuous_inputs, contextual_embedding])\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            mlp_input = mlp_layer(mlp_input)\n",
    "\n",
    "        return self.output_layer(mlp_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define your model instance\n",
    "tabTransformer = TabTransformer(\n",
    "    categories=[],        # No categorical features\n",
    "    num_continuous=26,    # Number of numerical features\n",
    "    dim=16,               # Embedding/transformer dimension\n",
    "    dim_out=1,            # Dimension of the model output (1 for binary classification)\n",
    "    depth=6,              # Number of transformer layers in the stack\n",
    "    heads=8,              # Number of attention heads\n",
    "    attn_dropout=0.1,     # Attention layer dropout in transformers\n",
    "    ff_dropout=0.1,       # Feed-forward layer dropout in transformers\n",
    "    mlp_hidden=[(32, 'relu'), (16, 'relu')]  # MLP layer dimensions and activations\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "tabTransformer.compile(optimizer='adam', \n",
    "              loss='mse',  # Example loss, use appropriate loss function for your task\n",
    "              metrics=['mae'])  # Example metric, use appropriate metrics for your task\n",
    "\n",
    "# Fit the model to your training data\n",
    "tabTransformer.fit(training_data_classes, epochs = 150, validation_data=(X_val_noisy, y_val_label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data with TEST_FRACTION Flag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Cloud type index = 37\n",
    "# clear = 0\n",
    "# stratus continental = 1\n",
    "# stratus maritime = 2\n",
    "# cumulus continental clean = 3\n",
    "# cumulus continental polluted = 4\n",
    "# cumulus maritime = 5\n",
    "# cirrus = 6\n",
    "# cirrus + water clouds (mixed) >= 7\n",
    "\n",
    "USE_MIXED = True\n",
    "USE_PRESSURE_TEMP = False\n",
    "USE_SPECIFIC_CLOUDS = False\n",
    "LOW_VIS_FRAC = False\n",
    "low_vis_frac_train = 0.05\n",
    "low_vis_threshold = 10000 #50000\n",
    "cloud_types = [6] #0,1,2 ::: 0, 3, 4\n",
    "noise_level = 0.01 # % of max\n",
    "dnn_batch_size = 64\n",
    "\n",
    "TEST_FRACTION = False\n",
    "\n",
    "if USE_MIXED:\n",
    "    filenames_vis = ['fog_data2_vis_clear.dat', 'fog_data2_vis_ice.dat', 'fog_data2_vis_mixed.dat', 'fog_data2_vis_water.dat']\n",
    "    filenames_ir = ['fog_data2_ir_clear.dat', 'fog_data2_ir_ice.dat', 'fog_data2_ir_mixed.dat', 'fog_data2_ir_water.dat']\n",
    "else:\n",
    "    filenames_vis = ['fog_data2_vis_clear.dat', 'fog_data2_vis_ice.dat', 'fog_data2_vis_water.dat']\n",
    "    filenames_ir = ['fog_data2_ir_clear.dat', 'fog_data2_ir_ice.dat', 'fog_data2_ir_water.dat']\n",
    "\n",
    "target_i = 30 # Visibility index\n",
    "if USE_PRESSURE_TEMP:\n",
    "    features_i = np.concatenate((np.arange(4, 17), np.arange(20, 30), np.arange(31,36))) # 13 ir bands, 10 vis bands, 3 angles, pressure, temp\n",
    "else:\n",
    "    features_i = np.concatenate((np.arange(4, 17), np.arange(20, 30), np.arange(31,34)))\n",
    "\n",
    "# Combine .dat files into one numpy array\n",
    "data_ir_vis = []\n",
    "for file_ir, file_vis in zip(filenames_ir, filenames_vis):\n",
    "  raw_data_ir = pd.read_csv('fog_dataset2/' + file_ir).to_numpy()\n",
    "  raw_data_vis = pd.read_csv('fog_dataset2/' + file_vis).to_numpy()\n",
    "\n",
    "  data_ir = np.array([row[0].split() for row in raw_data_ir[25:]])\n",
    "  data_vis = np.array([row[0].split()[1:] for row in raw_data_vis[41:]])\n",
    "\n",
    "  if len(data_ir_vis) == 0:\n",
    "    data_ir_vis = np.hstack([data_ir, data_vis])\n",
    "  else:\n",
    "    data_ir_vis = np.vstack([data_ir_vis, np.hstack([data_ir, data_vis])])\n",
    "\n",
    "# Remove surface description to convert to float\n",
    "data_ir_vis = data_ir_vis[:,:-1] \n",
    "data_ir_vis = data_ir_vis.astype(np.float32)\n",
    "\n",
    "# 19 Data points contains nan. Not sure why. Band 32 culprit, 8th feature\n",
    "nan_i = np.where(np.isnan(data_ir_vis))[0]\n",
    "data_ir_vis = np.delete(data_ir_vis, nan_i, axis=0)\n",
    "\n",
    "# Cloud type index = 37\n",
    "if USE_SPECIFIC_CLOUDS:\n",
    "   data_ir_vis = data_ir_vis[np.isin(data_ir_vis[:, 37], cloud_types)]\n",
    "\n",
    "\n",
    "X = data_ir_vis[:, features_i]\n",
    "y = data_ir_vis[:,target_i]\n",
    "\n",
    "noise_std_devs = np.zeros(features_i.shape)\n",
    "\n",
    "for i, feature_i in enumerate(features_i):\n",
    "    noise_std = (np.max(data_ir_vis[:,feature_i]) - np.min(data_ir_vis[:,feature_i])) * noise_level # 1% of mean or max??\n",
    "    noise_std_devs[i] = noise_std\n",
    "\n",
    "# Scaling features and noise standard deviations\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "noise_stds_scaled = np.divide(noise_std_devs, feature_scaler.scale_)\n",
    "\n",
    "# Scaling target\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "if not TEST_FRACTION:\n",
    "    # First split: Separate out a test set (5% of the original dataset)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_scaled, test_size=(1/20))\n",
    "\n",
    "\n",
    "if LOW_VIS_FRAC:\n",
    "    if not TEST_FRACTION:\n",
    "        low_vis_i = np.where(target_scaler.inverse_transform(y_temp) < low_vis_threshold)[0]\n",
    "        high_vis_i = np.where(target_scaler.inverse_transform(y_temp) >= low_vis_threshold)[0]\n",
    "    else:\n",
    "        low_vis_i = np.where(target_scaler.inverse_transform(y_scaled) < low_vis_threshold)[0]\n",
    "        high_vis_i = np.where(target_scaler.inverse_transform(y_scaled) >= low_vis_threshold)[0]\n",
    "\n",
    "\n",
    "    n_high_vis = int(len(low_vis_i) / low_vis_frac_train * (1 - low_vis_frac_train))\n",
    "    print(f'real frac: {len(low_vis_i)/len(y_scaled)}')\n",
    "    if n_high_vis > len(high_vis_i):\n",
    "        n_high_vis = len(high_vis_i)\n",
    "        n_low_vis = int(low_vis_frac_train * n_high_vis)\n",
    "        r_low_vis_i = np.random.choice(low_vis_i, n_low_vis, replace=False)\n",
    "        low_vis_i = r_low_vis_i\n",
    "    print(f'n_low_vis: {len(low_vis_i)}. n_high_vis: {n_high_vis}')\n",
    "    r_high_vis_i = np.random.choice(high_vis_i, n_high_vis, replace=False)\n",
    "    if not TEST_FRACTION:\n",
    "        X_temp = np.vstack([X_temp[low_vis_i], X_temp[r_high_vis_i]])\n",
    "        y_temp = np.vstack([y_temp[low_vis_i], y_temp[r_high_vis_i]])\n",
    "    else:\n",
    "        X_temp = np.vstack([X_scaled[low_vis_i], X_scaled[r_high_vis_i]])\n",
    "        y_temp = np.vstack([y_scaled[low_vis_i], y_scaled[r_high_vis_i]])\n",
    "\n",
    "\n",
    "if TEST_FRACTION:\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X_temp, y_temp, test_size=(1/19))\n",
    "  \n",
    "\n",
    "# Second split: Split the remaining data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=(1/19))\n",
    "\n",
    "# Create noisy data\n",
    "X_train_noisy = X_train + np.multiply(np.random.normal(0, 1, X_train.shape), noise_stds_scaled)\n",
    "X_val_noisy = X_val + np.multiply(np.random.normal(0, 1, X_val.shape), noise_stds_scaled)\n",
    "X_test_noisy = X_test + np.multiply(np.random.normal(0, 1, X_test.shape), noise_stds_scaled)\n",
    "\n",
    "# Create class labels\n",
    "y_train_label = np.where(target_scaler.inverse_transform(y_train) < low_vis_threshold, 1, 0)\n",
    "y_val_label = np.where(target_scaler.inverse_transform(y_val) < low_vis_threshold, 1, 0)\n",
    "y_test_label = np.where(target_scaler.inverse_transform(y_test) < low_vis_threshold, 1, 0)\n",
    "\n",
    "# Create training data for DNN that adds noise to batches\n",
    "training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size=dnn_batch_size)\n",
    "training_data_classes = tf.data.Dataset.from_tensor_slices((X_train, y_train_label)).batch(batch_size=dnn_batch_size)\n",
    "\n",
    "def noise_map(features, labels):\n",
    "    noise = tf.random.normal(shape=tf.shape(features))\n",
    "    noise = noise * noise_stds_scaled\n",
    "    noisy_features = features + noise\n",
    "    return noisy_features, labels\n",
    "\n",
    "training_data = training_data.map(noise_map)\n",
    "training_data_classes = training_data_classes.map(noise_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QRNN (Not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import quantnn as q\n",
    "\n",
    "quantiles=[0.1, 0.5, 0.9]\n",
    "layers = 4\n",
    "neurons = 128\n",
    "activation = \"relu\"\n",
    "model = (layers, neurons, activation)\n",
    "qrnn = q.QRNN(quantiles, n_inputs=1, model=model)\n",
    "training_data = (X_train, y_train)\n",
    "# Model setup\n",
    "model = qrnn(quantiles=[0.1, 0.5, 0.9], n_inputs=len(features_i))  # Estimate 10th, 50th, 90th percentiles\n",
    "results = qrnn.train(training_data=training_data, n_epochs=15)\n",
    "\n",
    "# Model fitting\n",
    "model.train(X_train, y_train, epochs=100)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test) \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typhon.retrieval.qrnn.qrnn import QRNN\n",
    "\n",
    "# 2. Create QRNN Model (Same as before)\n",
    "model = QRNN(\n",
    "    quantiles=[0.1, 0.5, 0.9],  \n",
    "    input_dimensions=X_train.shape[1],\n",
    "    model=(3, 128, 'relu'),\n",
    "    initial_learning_rate = 0.01\n",
    ")\n",
    "\n",
    "training_data = (X_train, y_train)\n",
    "validation_data = (X_val, y_val)\n",
    "\n",
    "# 3. Train the Model (Updated)\n",
    "model.train(training_data, validation_data, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_n_samples = np.zeros_like(random_fractions, dtype=int)\n",
    "n_fillers = 0\n",
    "for cloud_i in range(len(random_fractions)):\n",
    "    n_samples = round(len(low_vis_i) * random_fractions[cloud_i])\n",
    "    if n_samples > len(possible_indicies):\n",
    "        n_fillers = n_samples - len(possible_indicies)\n",
    "    random_n_samples[cloud_i] = n_samples + n_fillers\n",
    "    n_fillers = 0\n",
    "# Fixa först alla n_samples för alla cloud types.\n",
    "# Sen kolla om någon innehåller för många samples.\n",
    "# I så fall sätt dom till = len(possible_samples)\n",
    "# Kolla där det finns plats att lägga filler points, och lägg randomly ut dom där.\n",
    "cloud_type_fractions[subset_i] = random_fractions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
